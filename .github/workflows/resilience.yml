name: otel-demo resilience study (compose)

on:
  push:
    branches: [ main, master ]
  workflow_dispatch: {}

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      max-parallel: 20
      matrix:
        p_fail: [ "0.1", "0.3", "0.5", "0.7", "0.9" ]
        chunk: [ "1", "2", "3", "4" ]

    env:
      ENVOY_PORT: "8080"
      LOOKBACK_MINUTES: "30"
      WARMUP_SECONDS: "120"
      CHAOS_WINDOW_SECONDS: "60"
      CHAOS_MEASURE_DELAY_SECONDS: "15"
      CHAOS_MEASURE_WINDOW_SECONDS: "40"
      LATENCY_P95_THRESHOLD_MS: "1500"
      WINDOWS: "100"
      SAMPLES: "120000"
      OTEL_DEMO_REF: "main"
      VALIDATION_WINDOW_SECONDS: "60"
      VALIDATION_COLLECT_WINDOW_SECONDS: "60"
      VALIDATION_COLLECT_DELAY_SECONDS: "15"
      VALIDATION_MIN_TOTAL: "80"
      VALIDATION_MAX_ATTEMPTS: "3"
      VALIDATION_RETRY_SLEEP: "5"
      VALIDATION_MAX_LIVE: "0.99"
      VALIDATION_PROBE_URL: "http://localhost:8080/"
      VALIDATION_PROBE_INTERVAL: "1"
      VALIDATION_MIN_PROBE_FAILURES: "1"
      HEALTH_CHECK_WINDOW_SECONDS: "10"
      HEALTH_CHECK_PROBE_ATTEMPTS: "2"
      LOCUST_USERS: "150"
      LOCUST_SPAWN_RATE: "30"

    steps:
      - uses: actions/checkout@v4

      - name: Install jq
        run: |
          sudo apt-get update -y
          sudo apt-get install -y jq

      - name: Print Docker versions
        run: |
          docker --version
          docker compose version || true

      - name: Fetch opentelemetry-demo
        run: bash vendor/fetch-otel-demo.sh "$OTEL_DEMO_REF"

      - name: Bring up the demo (Compose)
        working-directory: vendor/opentelemetry-demo
        run: |
          docker compose up --force-recreate --remove-orphans --detach
          bash ../../scripts/wait_http.sh "http://localhost:${ENVOY_PORT}/" 180
          bash ../../scripts/wait_http.sh "http://localhost:${ENVOY_PORT}/jaeger/ui" 180
          bash ../../scripts/wait_http.sh "http://localhost:${ENVOY_PORT}/loadgen/" 180

      - name: Warm up & ensure trace traffic
        run: |
          python3 scripts/warmup.py --locust "http://localhost:${ENVOY_PORT}/loadgen" --timeout 300

      - name: Baseline health check (no chaos)
        run: |
          python3 scripts/collect_live.py \
            --locust "http://localhost:${ENVOY_PORT}/loadgen" \
            --window "${HEALTH_CHECK_WINDOW_SECONDS}" \
            --latency-p95-threshold "${LATENCY_P95_THRESHOLD_MS}" \
            --probe-frontend "http://localhost:${ENVOY_PORT}" \
            --probe-attempts "${HEALTH_CHECK_PROBE_ATTEMPTS}" \
            --window-log "" \
            --out health_baseline.json
          python3 - <<'PY'
          import json, sys
          data=json.load(open('health_baseline.json'))
          r=data.get('R_live',0)
          detail=data.get('detail',{})
          if r < 0.95:
              print('Baseline health check failed: R_live',r,'detail',detail, file=sys.stderr)
              sys.exit(1)
          print('Baseline R_live OK:', r)
          PY

      - name: Validate chaos + live measurement
        id: chaos_validation
        run: |
          python3 scripts/validate_chaos_live.py \
            --locust "http://localhost:${ENVOY_PORT}/loadgen" \
            --allowlist config/services_allowlist.txt \
            --window "${VALIDATION_WINDOW_SECONDS}" \
            --collect-window "${VALIDATION_COLLECT_WINDOW_SECONDS}" \
            --collect-delay "${VALIDATION_COLLECT_DELAY_SECONDS}" \
            --latency-p95-threshold "${LATENCY_P95_THRESHOLD_MS}" \
            --log validation_window_log.jsonl \
            --live validation_live.json \
            --summary validation_summary.json \
            --min-kills 1 \
            --min-total "${VALIDATION_MIN_TOTAL}" \
            --max-attempts "${VALIDATION_MAX_ATTEMPTS}" \
            --retry-sleep "${VALIDATION_RETRY_SLEEP}" \
            --max-live "${VALIDATION_MAX_LIVE}" \
            --probe-url "${VALIDATION_PROBE_URL}" \
            --probe-interval "${VALIDATION_PROBE_INTERVAL}" \
            --min-probe-failures "${VALIDATION_MIN_PROBE_FAILURES}" \
            --probe-frontend "http://localhost:${ENVOY_PORT}" \
            --probe-attempts 2 || echo "validation_status=$?" >> "$GITHUB_OUTPUT"

      - name: Annotate validation status
        if: steps.chaos_validation.outputs.validation_status && steps.chaos_validation.outputs.validation_status != '0'
        run: |
          echo "::warning::Chaos validation exited with status ${{ steps.chaos_validation.outputs.validation_status }}. See validation_summary.json for details." \
            | tee /tmp/chaos_warning

      - name: Build deps from traces (no /dependencies)
        run: |
          python3 scripts/traces_to_deps.py > deps.json
          test -s deps.json
          # Must have at least one edge
          [ "$(jq 'length' deps.json)" -gt 0 ] || (echo "deps.json empty" >&2; exit 1)

      - name: Build graph.json (deps â†’ G)
        run: |
          python3 scripts/deps_to_graph.py \
            --deps deps.json \
            --entrypoints config/entrypoints.txt \
            --out graph.json
          head -c 800 graph.json || true
          # Non-empty graph check
          [ "$(jq '.edges|length' graph.json)" -gt 0 ]
          

      - name: Measure replicas (actual Compose)
        run: bash scripts/read_replicas.sh > replicas.json

      - name: Run Monte-Carlo estimator (R_model)
        run: |
          python3 scripts/resilience.py \
            --graph graph.json --replicas replicas.json \
            --p "${{ matrix.p_fail }}" --samples "${SAMPLES}" \
            --out model_${{ matrix.p_fail }}.json

      - name: Chaos windows + R_live from Locust
        env:
          P_FAIL: ${{ matrix.p_fail }}
          CHUNK_ID: ${{ matrix.chunk }}
        run: |
          python3 -m pip install --quiet requests
          CHUNK=${CHUNK_ID}
          WINDOW_LOG="window_log_p${P_FAIL}_chunk${CHUNK}.jsonl"
          : > "$WINDOW_LOG"
          for i in $(seq 1 "${WINDOWS}"); do
            echo "::group::window chunk=${CHUNK} idx=$i"
            bash scripts/compose_chaos.sh "${P_FAIL}" config/services_allowlist.txt "${CHAOS_WINDOW_SECONDS}" "$WINDOW_LOG" &
            CHAOS_PID=$!
            sleep "${CHAOS_MEASURE_DELAY_SECONDS}"
            python3 scripts/collect_live.py \
              --locust "http://localhost:${ENVOY_PORT}/loadgen" \
              --window "${CHAOS_MEASURE_WINDOW_SECONDS}" \
              --latency-p95-threshold "${LATENCY_P95_THRESHOLD_MS}" \
              --probe-frontend "http://localhost:${ENVOY_PORT}" \
              --probe-attempts 2 \
              --window-log "$WINDOW_LOG" \
              --out live_${P_FAIL}_chunk${CHUNK}_${i}.json
            wait $CHAOS_PID || true
            echo "::endgroup::"
          done

      - name: Summarize chunk
        env:
          P_FAIL: ${{ matrix.p_fail }}
          CHUNK_ID: ${{ matrix.chunk }}
        run: |
          python3 - << 'PY'
          import glob, json, os
          p=os.environ["P_FAIL"]
          chunk=os.environ["CHUNK_ID"]
          files=sorted(glob.glob(f"live_{p}_chunk{chunk}_*.json"))
          vals=[json.load(open(f))["R_live"] for f in files]
          summary={
              "p_fail": float(p),
              "chunk": int(chunk),
              "windows": len(vals),
              "sum": sum(vals),
              "sum_sq": sum(v*v for v in vals)
          }
          json.dump(summary, open(f"summary_{p}_chunk{chunk}.json","w"))
          print(summary)
          PY

      - name: Print quick report
        run: |
          python3 - << 'PY'
          import glob,json,statistics as S
          rows=[]
          for f in sorted(glob.glob("summary_*.json")):
              s=json.load(open(f))
              rows.append((s["p_fail"], s["R_model"], s["R_live_mean"], s["R_live_sd"], s["windows"]))
          print("p\tR_model\tR_live_mean\tR_live_sd\twindows")
          for p,rm,rl,sd,w in sorted(rows): print(f"{p:.1f}\t{rm:.4f}\t{(rl or 0):.4f}\t{sd:.4f}\t{w}")
          PY

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: results_${{ matrix.p_fail }}_${{ matrix.chunk }}
          path: |
            deps.json
            graph.json
            replicas.json
            model_${{ matrix.p_fail }}.json
            live_${{ matrix.p_fail }}_chunk${{ matrix.chunk }}_*.json
            summary_${{ matrix.p_fail }}_chunk${{ matrix.chunk }}.json
            window_log_p${{ matrix.p_fail }}_chunk${{ matrix.chunk }}.jsonl
            validation_summary.json
            validation_live.json
            validation_window_log.jsonl

      - name: Tear down
        if: always()
        working-directory: vendor/opentelemetry-demo
        run: docker compose down -v

  report:
    name: Quick report (aggregate)
    needs: run
    runs-on: ubuntu-latest
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: results_*
          merge-multiple: true
      - name: Print quick report and enforce monotonicity
        run: |
          python3 - << 'PY'
          import glob, json, sys, math
          chunk_files = sorted(glob.glob('summary_*_chunk*.json'))
          if not chunk_files:
              print('No chunk summary files found', file=sys.stderr)
              sys.exit(1)
          agg = {}
          for f in chunk_files:
              s = json.load(open(f))
              p = float(s['p_fail'])
              data = agg.setdefault(p, {'sum':0.0,'sum_sq':0.0,'n':0})
              data['sum'] += s['sum']
              data['sum_sq'] += s['sum_sq']
              data['n'] += s['windows']
          rows = []
          for p, data in agg.items():
              n = data['n']
              if n == 0:
                  mean = 0.0
                  sd = 0.0
              else:
                  mean = data['sum']/n
                  variance = max(0.0, data['sum_sq']/n - mean*mean)
                  sd = math.sqrt(variance)
              try:
                  rm = json.load(open(f"model_{p}.json"))['R_model']
              except Exception:
                  print('Missing model file for p', p, file=sys.stderr)
                  sys.exit(1)
              rows.append((p, rm, mean, sd, n))
              with open(f"summary_{p}.json", "w") as fh:
                  json.dump({
                      'p_fail': p,
                      'R_model': rm,
                      'R_live_mean': mean,
                      'R_live_sd': sd,
                      'windows': n
                  }, fh)
          rows.sort()
          print('p\tR_model\tR_live_mean\tR_live_sd\tN')
          ok_model = True
          ok_live = True
          prev_rm = None
          prev_rl = None
          for p, rm, rl, sd, n in rows:
              print(f"{p:.1f}\t{rm:.4f}\t{rl:.4f}\t{sd:.4f}\t{n}")
              if prev_rm is not None and rm > prev_rm + 1e-9:
                  ok_model = False
              if prev_rl is not None and rl > prev_rl + 1e-9:
                  ok_live = False
              prev_rm, prev_rl = rm, rl
          if not ok_model:
              print('Monotonicity check failed: R_model is not non-increasing with p_fail', file=sys.stderr)
              sys.exit(1)
          if not ok_live:
              print('Warning: R_live_mean not non-increasing with p_fail (informational only)', file=sys.stderr)
          PY
